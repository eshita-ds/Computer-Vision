{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b6c8c0-1e66-400c-9909-8606de298995",
   "metadata": {
    "id": "f5b6c8c0-1e66-400c-9909-8606de298995"
   },
   "source": [
    "## <font color='purple'> Assignment 04 - Transfer Learning and Bounding Boxes and YOLOV8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c185bdac-bbd3-46aa-82c8-b8eba42e5f56",
   "metadata": {
    "id": "c185bdac-bbd3-46aa-82c8-b8eba42e5f56"
   },
   "source": [
    "<b>Part1: Using available pre-trained models for object detection, conduct inference on a short video (5-10 seconds) of a street scene drawing bounding boxes around detected vehicles. </b>\n",
    "\n",
    "<b>Step 1.</b> Collect a source video. It may be necessary to divide the video into discrete image frames. </br>\n",
    "<b>Step 2.</b> Conduct inference on each frame of the video, drawing bounding boxes around detected vehicles.</br>\n",
    "<b>Step 3.</b> Format the results back into a video.</br>\n",
    "\n",
    "Use Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1087f69-ee0f-4d94-834d-c456d485e800",
   "metadata": {
    "id": "b1087f69-ee0f-4d94-834d-c456d485e800"
   },
   "source": [
    "<font color='blue'> I have used TorchVision pretrained model for Object Detection here. I'm using RetinaNet as its faster as compared to Faster R-CNN and since my use case is video instead of images, it is better suited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7ea96f-2d04-497a-afa5-0b63d0996ee6",
   "metadata": {
    "id": "0a7ea96f-2d04-497a-afa5-0b63d0996ee6"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "WeKbKwJBptfH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WeKbKwJBptfH",
    "outputId": "6707bfd6-d0df-4227-da20-6af6531f75c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b345bfc-e239-4879-b08d-dd59f036b0ca",
   "metadata": {
    "id": "2b345bfc-e239-4879-b08d-dd59f036b0ca"
   },
   "source": [
    "<font color='blue'> As a first step, I will be extraction frames as images from my raw video file. In this case the name of my video file is <b>LA_street_test.mp4</b> which is a short 7-8 second video of a busy LA street. The video has visible cars, buses, traffic lights and people, so its suited well for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e8413b-f7d8-450a-88d5-b3a9ef5758af",
   "metadata": {
    "id": "65e8413b-f7d8-450a-88d5-b3a9ef5758af"
   },
   "outputs": [],
   "source": [
    "# Function: Will be used for extraction frames from the raw video\n",
    "# Parameters: Path of the raw video and output folder\n",
    "\n",
    "def extract_frames(video_path, output_folder):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_path = os.path.join(output_folder, f'frame_{frame_count:04d}.jpg')\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f7235-eca0-47db-848a-9e97332cbae6",
   "metadata": {
    "id": "ab2f7235-eca0-47db-848a-9e97332cbae6"
   },
   "source": [
    "<font color='blue'> Start the frame extraction by calling the previously created function. The extracted frames will be available in the directory named <b>LA_street_frames</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7293a084-279e-47fa-b087-4825d75f7692",
   "metadata": {
    "id": "7293a084-279e-47fa-b087-4825d75f7692"
   },
   "outputs": [],
   "source": [
    "extract_frames('LA_street_test.mp4', 'LA_street_frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68309e2b-0185-4e38-8753-b59b1fab9839",
   "metadata": {
    "id": "68309e2b-0185-4e38-8753-b59b1fab9839"
   },
   "source": [
    "<font color='blue'> Next loading the pretrained \"fasterrcnn_resnet50_fpn\" model from the torchvision object detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6512f9b3-f8a9-4a64-84f4-d34a273f8a7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6512f9b3-f8a9-4a64-84f4-d34a273f8a7d",
    "outputId": "81779a06-dd75-4bf6-d3e8-7d342fe37624"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:02<00:00, 78.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "model.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb6bc5-1c7b-499b-8aa6-4c3f33cd72c9",
   "metadata": {
    "id": "fedb6bc5-1c7b-499b-8aa6-4c3f33cd72c9"
   },
   "source": [
    "<font color='blue'> We will need to perform inference and visualization on the extracted frames. So we will need a list of label names extracted from the COCO2017 dataset. We referenced the list in text format from the following GitHub URL and converted that into a simple list object.\n",
    "\n",
    "<b> https://github.com/amikelive/coco-labels/blob/master/coco-labels-paper.txt\n",
    "\n",
    "<font color='blue'> <b> Note: </b> The COCO2014 dataset had 80 objects, so while using the models for evaluation, I was getting index out of bound errors.  The latest models under torchvision have been trained on COCO2017 dataset which has 90 objects. So I have referenced the list from the above URL to prevent issues during evaluations.\n",
    "\n",
    "<font color='blue'> Also including the \"_background _\" class as labels in torchvision models start at index 0 which is reserved for background class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54780a53-eb00-477a-bdba-dc7e58238fc4",
   "metadata": {
    "id": "54780a53-eb00-477a-bdba-dc7e58238fc4"
   },
   "outputs": [],
   "source": [
    "file_path = 'coco-labels-paper.txt'\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\"__background__\"]\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        category_name = line.strip()\n",
    "        COCO_INSTANCE_CATEGORY_NAMES.append(category_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63a0783-8ec9-4240-90f3-254ff57434fc",
   "metadata": {
    "id": "e63a0783-8ec9-4240-90f3-254ff57434fc"
   },
   "source": [
    "<font color='blue'> Defining the Color list to be used for our bounding boxes. I'm using Green and White for easy viewing and creating a dictionary assigning colors to each category label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead97118-da91-46a7-8778-5ed960df3cc0",
   "metadata": {
    "id": "ead97118-da91-46a7-8778-5ed960df3cc0"
   },
   "outputs": [],
   "source": [
    "# White: 255, 255, 255\n",
    "# Green: 0, 255, 0\n",
    "colors = [(255, 255, 255), (0, 255, 0)]\n",
    "\n",
    "class_colors = {i: colors[i % len(colors)] for i in range(len(COCO_INSTANCE_CATEGORY_NAMES))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e318c0f-1a12-4853-a38e-6e9b4908c7e5",
   "metadata": {
    "id": "9e318c0f-1a12-4853-a38e-6e9b4908c7e5"
   },
   "source": [
    "<font color='blue'> The following function will be used to perform the inference on the individual frames and will help with the bounding boxes and label assignment for detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a814d3b7-2b04-4c22-b7cf-6556cef1085b",
   "metadata": {
    "id": "a814d3b7-2b04-4c22-b7cf-6556cef1085b"
   },
   "outputs": [],
   "source": [
    "def process_frame(frame, model):\n",
    "    frame_tensor = F.to_tensor(frame).unsqueeze(0).to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frame_tensor)\n",
    "\n",
    "    boxes = outputs[0]['boxes'].cpu().numpy()\n",
    "    scores = outputs[0]['scores'].cpu().numpy()\n",
    "    labels = outputs[0]['labels'].cpu().numpy()\n",
    "\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        if score > 0.5:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            if label < len(COCO_INSTANCE_CATEGORY_NAMES):\n",
    "                label_name = COCO_INSTANCE_CATEGORY_NAMES[int(label)]\n",
    "                label_text = f'{label_name}: {score:.2f}'\n",
    "                color = class_colors[int(label)]\n",
    "\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            else:\n",
    "                print(f\"Warning: Label {label} is out of range.\")\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b25718-b0b7-42d5-82d2-a5cb67743787",
   "metadata": {
    "id": "21b25718-b0b7-42d5-82d2-a5cb67743787"
   },
   "source": [
    "<font color='blue'> Function that will use the Extracted frames as input, perform inference by calling the above function <b>process_frame</b> and then finally stich the frames together to generate the final output video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56202a7a-1e7b-4406-a68d-a953d0c4d24f",
   "metadata": {
    "id": "56202a7a-1e7b-4406-a68d-a953d0c4d24f"
   },
   "outputs": [],
   "source": [
    "def generate_video_from_frames(input_folder, output_video_path, model, fps=30):\n",
    "    frame_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg')]\n",
    "    frame_files.sort()\n",
    "\n",
    "    first_frame = cv2.imread(os.path.join(input_folder, frame_files[0]))\n",
    "    height, width, layers = first_frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(input_folder, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is not None:\n",
    "            processed_frame = process_frame(frame, model)\n",
    "            out.write(processed_frame)\n",
    "\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031bd46-673c-4d51-8f23-11b6b6675350",
   "metadata": {
    "id": "7031bd46-673c-4d51-8f23-11b6b6675350"
   },
   "source": [
    "<font color='blue'> Start Processing for Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbca7dae-b1d8-4b90-99f2-57bbaa28f702",
   "metadata": {
    "id": "dbca7dae-b1d8-4b90-99f2-57bbaa28f702"
   },
   "outputs": [],
   "source": [
    "input_folder = 'LA_street_frames'\n",
    "output_video_path = 'LA_street_output.mp4'\n",
    "generate_video_from_frames(input_folder, output_video_path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd9c6a-287f-445e-9a0a-ba7452f66c11",
   "metadata": {
    "id": "96cd9c6a-287f-445e-9a0a-ba7452f66c11"
   },
   "source": [
    "<font color='blue'> End Processing for Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eef807-601a-4a82-a53c-09c0be742b17",
   "metadata": {
    "id": "e4eef807-601a-4a82-a53c-09c0be742b17"
   },
   "source": [
    "### <font color='purple'> Trying out the same evaluation using Detectron2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2100e08c-720a-4447-b147-6b26a2acf744",
   "metadata": {
    "id": "2100e08c-720a-4447-b147-6b26a2acf744"
   },
   "source": [
    "<font color='blue'> Pre-Requisite: Install Detectron2 using the following command.\n",
    "\n",
    "<b> pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hR7rcKUlr8mM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hR7rcKUlr8mM",
    "outputId": "301b83d4-d2d5-41f1-986d-b5f227a871ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-7v280a7y\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-7v280a7y\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit ebe8b45437f86395352ab13402ba45b75b4d1ddb\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
      "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.5)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.17.0)\n",
      "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
      "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting black (from detectron2==0.6)\n",
      "  Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
      "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.3.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.12.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.64.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (71.0.4)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.5)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
      "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6338626 sha256=d23a23378d4bfa8af1e23eed3472f872dbfad274b88bf7bd2c68a1055e3996a2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qcdu288l/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61395 sha256=8de6aa195391a2b162b34bfd741b3062e56adddf5f536e8393d1c5e12bf95ff9\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=d168216da998f2dce71a7ac97119de5321c3243868f3b97b278fd38ee84b5975\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 black-24.8.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.10.1 yacs-0.1.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "55d2351ad9e44d94b91e529f45940c9d",
       "pip_warning": {
        "packages": [
         "pydevd_plugins"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a421c64-766b-4368-99f0-d4658bf0a32a",
   "metadata": {
    "id": "5a421c64-766b-4368-99f0-d4658bf0a32a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039b77f-4372-473f-8b9a-571d68f6924d",
   "metadata": {
    "id": "5039b77f-4372-473f-8b9a-571d68f6924d"
   },
   "source": [
    "<font color='Blue'> Loaded the pretrained Detectron2 models from the already installed library. Used the MODEL_DEVICE as cpu to force the model to use CPU configuration. Model configuration was giving errors related to CUDA as default configuration expects a GPU. So we force the configuration to use CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e3edc37-4861-4b30-a828-c7d7ebfa57ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e3edc37-4861-4b30-a828-c7d7ebfa57ae",
    "outputId": "58392a59-9dee-4eb8-85af-3ef0afe440b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_final_280758.pkl: 167MB [00:01, 140MB/s]                           \n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73b5e7-5de0-4adb-b95d-935357e1bf90",
   "metadata": {
    "id": "fb73b5e7-5de0-4adb-b95d-935357e1bf90"
   },
   "source": [
    "<font color='blue'> Redefine the list for COCO category names as the index starts from 1 for Detectron2 Model as compared to Torchvision that starts at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efab2085-5df5-4516-9323-5a5124525273",
   "metadata": {
    "id": "efab2085-5df5-4516-9323-5a5124525273"
   },
   "outputs": [],
   "source": [
    "file_path = 'coco-labels-paper.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    COCO_INSTANCE_CATEGORY_NAMES_DT2 = [line.strip() for line in file.readlines()]\n",
    "\n",
    "colors_DT2 = [(255, 255, 255), (0, 255, 0)]\n",
    "class_colors_DT2 = {i: colors_DT2[i % len(colors_DT2)] for i in range(len(COCO_INSTANCE_CATEGORY_NAMES_DT2))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7403a6-5ab8-46b1-9f9b-e793975de24f",
   "metadata": {
    "id": "1c7403a6-5ab8-46b1-9f9b-e793975de24f"
   },
   "source": [
    "<font color='blue'> Redefine the process_frame function to use the detectron2 model and new Category label list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8141b59-cba8-4e50-9922-482ef72945bd",
   "metadata": {
    "id": "a8141b59-cba8-4e50-9922-482ef72945bd"
   },
   "outputs": [],
   "source": [
    "def process_frame_detectron(frame, predictor):\n",
    "    outputs = predictor(frame)\n",
    "    instances = outputs[\"instances\"].to(\"cuda\")\n",
    "\n",
    "    boxes = instances.pred_boxes.tensor.numpy()\n",
    "    scores = instances.scores.numpy()\n",
    "    labels = instances.pred_classes.numpy()\n",
    "\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        if score > 0.5:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "            if label < len(COCO_INSTANCE_CATEGORY_NAMES_DT2):\n",
    "                label_name = COCO_INSTANCE_CATEGORY_NAMES_DT2[int(label)]\n",
    "                label_text = f'{label_name}: {score:.2f}'\n",
    "                color = class_colors_DT2[int(label)]\n",
    "\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            else:\n",
    "                print(f\"Warning: Label {label} is out of range.\")\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7887cd-4483-446f-b3e7-93e411a8832c",
   "metadata": {
    "id": "ea7887cd-4483-446f-b3e7-93e411a8832c"
   },
   "source": [
    "<font color='blue'> Redefine the Output video function to use the detectron2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6591c75-5e84-429e-945e-6e9540bfb881",
   "metadata": {
    "id": "b6591c75-5e84-429e-945e-6e9540bfb881"
   },
   "outputs": [],
   "source": [
    "def generate_video_from_frames_detectron(input_folder, output_video_path, predictor, fps=30):\n",
    "    frame_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg')]\n",
    "    frame_files.sort()\n",
    "\n",
    "    first_frame = cv2.imread(os.path.join(input_folder, frame_files[0]))\n",
    "    height, width, layers = first_frame.shape\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(input_folder, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is not None:\n",
    "            processed_frame = process_frame_detectron(frame, predictor)\n",
    "            out.write(processed_frame)\n",
    "\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1184a-4d8b-4324-ba93-30358f6cd530",
   "metadata": {
    "id": "f5b1184a-4d8b-4324-ba93-30358f6cd530"
   },
   "source": [
    "<font color='blue'> Start Processing using the Detectron 2 models, We will use the same set of frames extracted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af2aa6f4-6c61-4803-8d20-418de2b27671",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af2aa6f4-6c61-4803-8d20-418de2b27671",
    "outputId": "ba6661e6-8f72-4328-a56d-23b21fa9cfb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "input_folder = 'LA_street_frames'\n",
    "output_video_path = 'LA_street_output_detectron.mp4'\n",
    "generate_video_from_frames_detectron(input_folder, output_video_path, predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703138f-6b87-4c88-bbc8-7821de1656fc",
   "metadata": {
    "id": "4703138f-6b87-4c88-bbc8-7821de1656fc"
   },
   "source": [
    "<font color='blue'> End Processing for the frames using Detectron 2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971499c5-ed3c-4f88-8b4b-5286cf33c952",
   "metadata": {
    "id": "971499c5-ed3c-4f88-8b4b-5286cf33c952"
   },
   "source": [
    "#### <font color = 'green'> Part 2 in another Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42406e0e-13f1-4668-8ee6-1149e89a478d",
   "metadata": {
    "id": "42406e0e-13f1-4668-8ee6-1149e89a478d"
   },
   "source": [
    "## <font color='purple'> Thank You"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
